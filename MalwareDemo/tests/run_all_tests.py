#!/usr/bin/env python3
"""
Comprehensive Test Runner for Android Security Research Demo

Master test runner that orchestrates all testing components including
unit tests, integration tests, performance tests, and generates
comprehensive reports for the cybersecurity education platform.

EDUCATIONAL PURPOSE ONLY - For validating security demonstration functionality
"""

import os
import sys
import time
import json
import logging
import argparse
import subprocess
import threading
from pathlib import Path
from typing import Dict, List, Optional
from datetime import datetime
import importlib.util

# Add tests directory to path
sys.path.insert(0, str(Path(__file__).parent))

# Import test modules
from test_framework import TestRunner
from unit_tests import *
from performance_tests import PerformanceReportGenerator, ServicePerformanceTester, C2ServerLoadTester
from integration_tests import IntegrationTestRunner

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('comprehensive_test_results.log'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

class ComprehensiveTestSuite:
    """Master test suite coordinator"""
    
    def __init__(self, device_id: Optional[str] = None, output_dir: str = "test_results"):
        self.device_id = device_id
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        self.test_results = {
            "unit_tests": {},
            "integration_tests": {},
            "performance_tests": {},
            "attack_validation": {},
            "defense_validation": {}
        }
        
        self.start_time = None
        self.end_time = None
        
        logger.info(f"Initialized comprehensive test suite")
        logger.info(f"Device ID: {device_id}")
        logger.info(f"Output directory: {output_dir}")
    
    def run_all_tests(self, test_types: List[str] = None) -> Dict[str, any]:
        """Run all specified test types"""
        if test_types is None:
            test_types = ["unit", "integration", "performance", "attack", "defense"]
        
        self.start_time = time.time()
        logger.info("=" * 80)
        logger.info("STARTING COMPREHENSIVE ANDROID SECURITY DEMO TEST SUITE")
        logger.info("=" * 80)
        
        try:
            # Run unit tests
            if "unit" in test_types:
                logger.info("Running unit tests...")
                self.test_results["unit_tests"] = self._run_unit_tests()
            
            # Run integration tests (requires device)
            if "integration" in test_types and self.device_id:
                logger.info("Running integration tests...")
                self.test_results["integration_tests"] = self._run_integration_tests()
            
            # Run performance tests (requires device)
            if "performance" in test_types and self.device_id:
                logger.info("Running performance tests...")
                self.test_results["performance_tests"] = self._run_performance_tests()
            
            # Run attack validation tests (requires device)
            if "attack" in test_types and self.device_id:
                logger.info("Running attack validation tests...")
                self.test_results["attack_validation"] = self._run_attack_validation_tests()
            
            # Run defense validation tests (requires device)
            if "defense" in test_types and self.device_id:
                logger.info("Running defense validation tests...")
                self.test_results["defense_validation"] = self._run_defense_validation_tests()
            
            self.end_time = time.time()
            
            # Generate comprehensive report
            comprehensive_report = self._generate_comprehensive_report()
            
            # Save results
            self._save_test_results(comprehensive_report)
            
            # Print summary
            self._print_test_summary(comprehensive_report)
            
            return comprehensive_report
            
        except Exception as e:
            logger.error(f"Test suite execution failed: {e}")
            raise
    
    def _run_unit_tests(self) -> Dict[str, any]:
        """Run unit tests"""
        logger.info("Executing unit test suite...")
        
        try:
            # Run unit tests using unittest
            start_time = time.time()
            
            # Import and run unit tests
            import unittest
            
            # Create test suite
            test_suite = unittest.TestSuite()
            
            # Add test classes
            test_classes = [
                TestStealthServiceValidation,
                TestDataExfiltrationValidation,
                TestKeyloggerValidation,
                TestSilentInstallationValidation,
                TestSocialEngineeringValidation,
                TestPersistenceValidation,
                TestDefenseSystemValidation,
                TestC2ServerValidation,
                TestIntegrationScenarios,
                TestPerformanceValidation
            ]
            
            for test_class in test_classes:
                tests = unittest.TestLoader().loadTestsFromTestCase(test_class)
                test_suite.addTests(tests)
            
            # Run tests
            stream = open(self.output_dir / "unit_tests.log", 'w')
            runner = unittest.TextTestRunner(
                stream=stream,
                verbosity=2,
                descriptions=True
            )
            
            result = runner.run(test_suite)
            stream.close()
            
            duration = time.time() - start_time
            
            unit_test_results = {
                "total_tests": result.testsRun,
                "passed": result.testsRun - len(result.failures) - len(result.errors),
                "failed": len(result.failures),
                "errors": len(result.errors),
                "success_rate": ((result.testsRun - len(result.failures) - len(result.errors)) / result.testsRun * 100) if result.testsRun > 0 else 0,
                "duration": duration,
                "failures": [{"test": str(test), "error": error} for test, error in result.failures],
                "errors": [{"test": str(test), "error": error} for test, error in result.errors]
            }
            
            logger.info(f"Unit tests completed: {unit_test_results['passed']}/{unit_test_results['total_tests']} passed")
            return unit_test_results
            
        except Exception as e:
            logger.error(f"Unit tests failed: {e}")
            return {"error": str(e), "success_rate": 0}
    
    def _run_integration_tests(self) -> Dict[str, any]:
        """Run integration tests"""
        logger.info("Executing integration test suite...")
        
        try:
            runner = IntegrationTestRunner(self.device_id)
            integration_results = runner.run_all_integration_tests()
            
            logger.info(f"Integration tests completed: {integration_results['summary']['success_rate']:.1f}% success rate")
            return integration_results
            
        except Exception as e:
            logger.error(f"Integration tests failed: {e}")
            return {"error": str(e), "summary": {"success_rate": 0}}
    
    def _run_performance_tests(self) -> Dict[str, any]:
        """Run performance tests"""
        logger.info("Executing performance test suite...")
        
        try:
            # Initialize performance report generator
            report_generator = PerformanceReportGenerator()
            
            # Service performance tests
            service_tester = ServicePerformanceTester(self.device_id)
            
            stealth_results = service_tester.test_stealth_service_performance()
            report_generator.add_test_results("stealth_service_performance", stealth_results)
            
            collection_results = service_tester.test_data_collection_performance()
            report_generator.add_test_results("data_collection_performance", collection_results)
            
            persistence_results = service_tester.test_persistence_overhead()
            report_generator.add_test_results("persistence_overhead", persistence_results)
            
            # C2 server load tests
            c2_tester = C2ServerLoadTester()
            
            registration_results = c2_tester.run_registration_load_test()
            report_generator.add_test_results("c2_registration_load_test", registration_results.__dict__)
            
            transmission_results = c2_tester.run_data_transmission_load_test()
            report_generator.add_test_results("c2_transmission_load_test", transmission_results.__dict__)
            
            # Generate performance report
            performance_report = report_generator.generate_performance_report()
            
            # Save performance charts
            chart_dir = self.output_dir / "performance_charts"
            report_generator.generate_performance_charts(str(chart_dir))
            
            logger.info("Performance tests completed successfully")
            return performance_report
            
        except Exception as e:
            logger.error(f"Performance tests failed: {e}")
            return {"error": str(e), "report_metadata": {"test_count": 0}}
    
    def _run_attack_validation_tests(self) -> Dict[str, any]:
        """Run attack validation tests"""
        logger.info("Executing attack validation test suite...")
        
        try:
            from test_framework import TestRunner
            
            runner = TestRunner(device_id=self.device_id)
            attack_results = runner.run_attack_tests_only()
            
            logger.info(f"Attack validation completed: {attack_results['summary']['success_rate']:.1f}% success rate")
            return attack_results
            
        except Exception as e:
            logger.error(f"Attack validation tests failed: {e}")
            return {"error": str(e), "summary": {"success_rate": 0}}
    
    def _run_defense_validation_tests(self) -> Dict[str, any]:
        """Run defense validation tests"""
        logger.info("Executing defense validation test suite...")
        
        try:
            from test_framework import TestRunner
            
            runner = TestRunner(device_id=self.device_id)
            defense_results = runner.run_defense_tests_only()
            
            logger.info(f"Defense validation completed: {defense_results['summary']['success_rate']:.1f}% success rate")
            return defense_results
            
        except Exception as e:
            logger.error(f"Defense validation tests failed: {e}")
            return {"error": str(e), "summary": {"success_rate": 0}}
    
    def _generate_comprehensive_report(self) -> Dict[str, any]:
        """Generate comprehensive test report"""
        total_duration = self.end_time - self.start_time if self.end_time and self.start_time else 0
        
        # Calculate overall statistics
        total_tests = 0
        total_passed = 0
        total_failed = 0
        
        # Unit tests
        if "unit_tests" in self.test_results and "total_tests" in self.test_results["unit_tests"]:
            unit_results = self.test_results["unit_tests"]
            total_tests += unit_results.get("total_tests", 0)
            total_passed += unit_results.get("passed", 0)
            total_failed += unit_results.get("failed", 0) + unit_results.get("errors", 0)
        
        # Integration tests
        if "integration_tests" in self.test_results and "summary" in self.test_results["integration_tests"]:
            int_results = self.test_results["integration_tests"]["summary"]
            total_tests += int_results.get("total_tests", 0)
            total_passed += int_results.get("successful", 0)
            total_failed += int_results.get("failed", 0)
        
        # Attack validation tests
        if "attack_validation" in self.test_results and "summary" in self.test_results["attack_validation"]:
            attack_results = self.test_results["attack_validation"]["summary"]
            total_tests += attack_results.get("total_tests", 0)
            total_passed += attack_results.get("passed", 0)
            total_failed += attack_results.get("failed", 0)
        
        # Defense validation tests
        if "defense_validation" in self.test_results and "summary" in self.test_results["defense_validation"]:
            defense_results = self.test_results["defense_validation"]["summary"]
            total_tests += defense_results.get("total_tests", 0)
            total_passed += defense_results.get("passed", 0)
            total_failed += defense_results.get("failed", 0)
        
        overall_success_rate = (total_passed / total_tests * 100) if total_tests > 0 else 0
        
        comprehensive_report = {
            "report_metadata": {
                "generated_at": datetime.now().isoformat(),
                "total_duration": total_duration,
                "device_id": self.device_id,
                "platform": "Android Security Research Demo",
                "test_framework_version": "1.0.0"
            },
            "executive_summary": {
                "total_tests": total_tests,
                "total_passed": total_passed,
                "total_failed": total_failed,
                "overall_success_rate": overall_success_rate,
                "test_categories": len([k for k, v in self.test_results.items() if v and not v.get("error")]),
                "critical_issues": self._count_critical_issues(),
                "recommendations": self._generate_recommendations()
            },
            "detailed_results": self.test_results,
            "performance_analysis": self._analyze_performance_results(),
            "security_assessment": self._analyze_security_results(),
            "recommendations": self._generate_detailed_recommendations()
        }
        
        return comprehensive_report
    
    def _count_critical_issues(self) -> int:
        """Count critical issues across all tests"""
        critical_issues = 0
        
        # Check unit test failures
        if "unit_tests" in self.test_results:
            unit_results = self.test_results["unit_tests"]
            critical_issues += unit_results.get("errors", 0)
        
        # Check integration test failures
        if "integration_tests" in self.test_results:
            int_results = self.test_results["integration_tests"]
            if "summary" in int_results:
                critical_issues += int_results["summary"].get("failed", 0)
        
        # Check performance issues
        if "performance_tests" in self.test_results:
            perf_results = self.test_results["performance_tests"]
            if "executive_summary" in perf_results:
                critical_issues += perf_results["executive_summary"].get("critical_issues", 0)
        
        return critical_issues
    
    def _generate_recommendations(self) -> List[str]:
        """Generate high-level recommendations"""
        recommendations = []
        
        # Analyze results and generate recommendations
        if "unit_tests" in self.test_results:
            unit_success_rate = self.test_results["unit_tests"].get("success_rate", 0)
            if unit_success_rate < 90:
                recommendations.append("Address unit test failures to ensure component reliability")
        
        if "integration_tests" in self.test_results:
            int_success_rate = self.test_results["integration_tests"].get("summary", {}).get("success_rate", 0)
            if int_success_rate < 80:
                recommendations.append("Fix integration test failures to ensure system-wide functionality")
        
        if "performance_tests" in self.test_results:
            perf_results = self.test_results["performance_tests"]
            if perf_results.get("executive_summary", {}).get("overall_performance") == "POOR":
                recommendations.append("Optimize performance bottlenecks identified in testing")
        
        if not recommendations:
            recommendations.append("All systems operating within acceptable parameters")
        
        return recommendations
    
    def _analyze_performance_results(self) -> Dict[str, any]:
        """Analyze performance test results"""
        if "performance_tests" not in self.test_results:
            return {"status": "not_run"}
        
        perf_results = self.test_results["performance_tests"]
        
        return {
            "overall_rating": perf_results.get("executive_summary", {}).get("overall_performance", "UNKNOWN"),
            "critical_issues": perf_results.get("executive_summary", {}).get("critical_issues", 0),
            "warnings": perf_results.get("executive_summary", {}).get("warnings", 0),
            "recommendations_count": len(perf_results.get("performance_recommendations", [])),
            "test_count": perf_results.get("report_metadata", {}).get("test_count", 0)
        }
    
    def _analyze_security_results(self) -> Dict[str, any]:
        """Analyze security-related test results"""
        security_analysis = {
            "attack_vector_coverage": 0,
            "defense_effectiveness": 0,
            "critical_vulnerabilities": 0,
            "security_rating": "UNKNOWN"
        }
        
        # Analyze attack validation results
        if "attack_validation" in self.test_results:
            attack_results = self.test_results["attack_validation"]
            if "summary" in attack_results:
                security_analysis["attack_vector_coverage"] = attack_results["summary"].get("success_rate", 0)
        
        # Analyze defense validation results
        if "defense_validation" in self.test_results:
            defense_results = self.test_results["defense_validation"]
            if "summary" in defense_results:
                security_analysis["defense_effectiveness"] = defense_results["summary"].get("success_rate", 0)
        
        # Calculate overall security rating
        avg_coverage = (security_analysis["attack_vector_coverage"] + security_analysis["defense_effectiveness"]) / 2
        
        if avg_coverage >= 90:
            security_analysis["security_rating"] = "EXCELLENT"
        elif avg_coverage >= 80:
            security_analysis["security_rating"] = "GOOD"
        elif avg_coverage >= 70:
            security_analysis["security_rating"] = "ACCEPTABLE"
        else:
            security_analysis["security_rating"] = "NEEDS_IMPROVEMENT"
        
        return security_analysis
    
    def _generate_detailed_recommendations(self) -> List[Dict[str, str]]:
        """Generate detailed recommendations with categories"""
        recommendations = []
        
        # Unit test recommendations
        if "unit_tests" in self.test_results:
            unit_results = self.test_results["unit_tests"]
            if unit_results.get("failed", 0) > 0:
                recommendations.append({
                    "category": "Code Quality",
                    "priority": "HIGH",
                    "recommendation": f"Fix {unit_results['failed']} failing unit tests",
                    "impact": "Ensures individual component reliability"
                })
        
        # Integration test recommendations
        if "integration_tests" in self.test_results:
            int_results = self.test_results["integration_tests"]
            if int_results.get("summary", {}).get("failed", 0) > 0:
                recommendations.append({
                    "category": "System Integration",
                    "priority": "HIGH", 
                    "recommendation": "Address integration test failures",
                    "impact": "Ensures end-to-end system functionality"
                })
        
        # Performance recommendations
        if "performance_tests" in self.test_results:
            perf_results = self.test_results["performance_tests"]
            perf_recs = perf_results.get("performance_recommendations", [])
            for rec in perf_recs:
                recommendations.append({
                    "category": "Performance",
                    "priority": "MEDIUM",
                    "recommendation": rec,
                    "impact": "Improves system efficiency and user experience"
                })
        
        # Security recommendations
        security_results = self._analyze_security_results()
        if security_results["security_rating"] in ["NEEDS_IMPROVEMENT", "ACCEPTABLE"]:
            recommendations.append({
                "category": "Security",
                "priority": "CRITICAL",
                "recommendation": "Improve security test coverage and defense effectiveness",
                "impact": "Ensures educational platform demonstrates effective security measures"
            })
        
        return recommendations
    
    def _save_test_results(self, comprehensive_report: Dict[str, any]):
        """Save comprehensive test results"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # Save main report
        main_report_file = self.output_dir / f"comprehensive_test_report_{timestamp}.json"
        try:
            with open(main_report_file, 'w') as f:
                json.dump(comprehensive_report, f, indent=2)
            logger.info(f"Comprehensive test report saved to {main_report_file}")
        except Exception as e:
            logger.error(f"Failed to save comprehensive report: {e}")
        
        # Save individual test results
        for test_type, results in self.test_results.items():
            if results and not results.get("error"):
                result_file = self.output_dir / f"{test_type}_results_{timestamp}.json"
                try:
                    with open(result_file, 'w') as f:
                        json.dump(results, f, indent=2)
                except Exception as e:
                    logger.error(f"Failed to save {test_type} results: {e}")
    
    def _print_test_summary(self, comprehensive_report: Dict[str, any]):
        """Print comprehensive test summary"""
        summary = comprehensive_report["executive_summary"]
        
        print("\n" + "=" * 80)
        print("COMPREHENSIVE TEST SUITE RESULTS")
        print("=" * 80)
        
        print(f"üìä Overall Statistics:")
        print(f"   Total Tests: {summary['total_tests']}")
        print(f"   Passed: {summary['total_passed']}")
        print(f"   Failed: {summary['total_failed']}")
        print(f"   Success Rate: {summary['overall_success_rate']:.1f}%")
        print(f"   Duration: {comprehensive_report['report_metadata']['total_duration']:.2f} seconds")
        
        print(f"\nüîç Test Coverage:")
        print(f"   Test Categories: {summary['test_categories']}")
        print(f"   Critical Issues: {summary['critical_issues']}")
        
        # Performance analysis
        perf_analysis = comprehensive_report["performance_analysis"]
        if perf_analysis.get("status") != "not_run":
            print(f"\n‚ö° Performance Analysis:")
            print(f"   Overall Rating: {perf_analysis['overall_rating']}")
            print(f"   Critical Issues: {perf_analysis['critical_issues']}")
            print(f"   Warnings: {perf_analysis['warnings']}")
        
        # Security analysis
        security_analysis = comprehensive_report["security_assessment"]
        print(f"\nüõ°Ô∏è Security Analysis:")
        print(f"   Attack Vector Coverage: {security_analysis['attack_vector_coverage']:.1f}%")
        print(f"   Defense Effectiveness: {security_analysis['defense_effectiveness']:.1f}%")
        print(f"   Security Rating: {security_analysis['security_rating']}")
        
        # Recommendations
        print(f"\nüí° Key Recommendations:")
        for i, rec in enumerate(summary['recommendations'][:3], 1):
            print(f"   {i}. {rec}")
        
        # Overall assessment
        print(f"\nüéØ Overall Assessment:")
        if summary['overall_success_rate'] >= 90:
            print("   ‚úÖ EXCELLENT - System ready for production use")
        elif summary['overall_success_rate'] >= 80:
            print("   ‚úÖ GOOD - Minor issues to address")
        elif summary['overall_success_rate'] >= 70:
            print("   ‚ö†Ô∏è ACCEPTABLE - Some issues need attention")
        else:
            print("   ‚ùå NEEDS WORK - Significant issues require fixing")
        
        print("=" * 80)

def main():
    """Main entry point for comprehensive test runner"""
    parser = argparse.ArgumentParser(
        description="Comprehensive Test Runner for Android Security Research Demo",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python run_all_tests.py --device emulator-5554 --all
  python run_all_tests.py --device emulator-5554 --unit --integration
  python run_all_tests.py --unit --performance --output custom_results
  python run_all_tests.py --device emulator-5554 --quick
        """
    )
    
    # Device configuration
    parser.add_argument("--device", help="Android device ID (required for device-based tests)")
    
    # Test type selection
    parser.add_argument("--all", action="store_true", help="Run all test types")
    parser.add_argument("--unit", action="store_true", help="Run unit tests")
    parser.add_argument("--integration", action="store_true", help="Run integration tests")
    parser.add_argument("--performance", action="store_true", help="Run performance tests")
    parser.add_argument("--attack", action="store_true", help="Run attack validation tests")
    parser.add_argument("--defense", action="store_true", help="Run defense validation tests")
    parser.add_argument("--quick", action="store_true", help="Run quick test suite (unit + basic validation)")
    
    # Output configuration
    parser.add_argument("--output", default="test_results", help="Output directory for test results")
    parser.add_argument("--verbose", action="store_true", help="Verbose logging output")
    
    # Analysis options
    parser.add_argument("--generate-charts", action="store_true", help="Generate performance charts")
    parser.add_argument("--export-reports", action="store_true", help="Export detailed reports")
    
    args = parser.parse_args()
    
    # Configure logging level
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    # Determine test types to run
    test_types = []
    
    if args.all:
        test_types = ["unit", "integration", "performance", "attack", "defense"]
    elif args.quick:
        test_types = ["unit", "attack", "defense"]
    else:
        if args.unit:
            test_types.append("unit")
        if args.integration:
            test_types.append("integration")
        if args.performance:
            test_types.append("performance")
        if args.attack:
            test_types.append("attack")
        if args.defense:
            test_types.append("defense")
    
    # Default to unit tests if nothing specified
    if not test_types:
        test_types = ["unit"]
        logger.info("No test types specified, defaulting to unit tests")
    
    # Validate device requirement
    device_required_tests = ["integration", "performance", "attack", "defense"]
    if any(test in test_types for test in device_required_tests) and not args.device:
        logger.error("Device ID required for integration, performance, attack, and defense tests")
        logger.error("Use --device <device_id> or run with --unit only")
        return 1
    
    try:
        # Initialize test suite
        test_suite = ComprehensiveTestSuite(
            device_id=args.device,
            output_dir=args.output
        )
        
        # Run tests
        logger.info(f"Running test types: {test_types}")
        comprehensive_report = test_suite.run_all_tests(test_types)
        
        # Additional reporting options
        if args.generate_charts and "performance" in test_types:
            logger.info("Generating performance charts...")
            # Charts would be generated by performance tests
        
        if args.export_reports:
            logger.info("Exporting detailed reports...")
            # Additional export functionality would go here
        
        # Determine exit code based on results
        success_rate = comprehensive_report["executive_summary"]["overall_success_rate"]
        critical_issues = comprehensive_report["executive_summary"]["critical_issues"]
        
        if success_rate >= 90 and critical_issues == 0:
            exit_code = 0  # Excellent
        elif success_rate >= 80 and critical_issues <= 2:
            exit_code = 0  # Good
        elif success_rate >= 70:
            exit_code = 1  # Acceptable but has issues
        else:
            exit_code = 2  # Needs significant work
        
        logger.info(f"Test suite completed with exit code {exit_code}")
        return exit_code
        
    except KeyboardInterrupt:
        logger.info("Test suite interrupted by user")
        return 130
    except Exception as e:
        logger.error(f"Test suite execution failed: {e}")
        return 1

if __name__ == "__main__":
    sys.exit(main())
