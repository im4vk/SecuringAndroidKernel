#!/usr/bin/env python3
"""
Performance Tests for Android Security Research Demo

Comprehensive performance testing suite for validating system efficiency,
resource usage, and scalability of the cybersecurity education platform.

EDUCATIONAL PURPOSE ONLY - For validating security demonstration performance
"""

import time
import psutil
import threading
import statistics
import subprocess
import json
import logging
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass
from datetime import datetime
import matplotlib.pyplot as plt
import numpy as np

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class PerformanceMetrics:
    """Performance measurement data structure"""
    cpu_usage: List[float]
    memory_usage: List[float]
    network_io: List[Tuple[int, int]]  # (bytes_sent, bytes_received)
    disk_io: List[Tuple[int, int]]     # (bytes_read, bytes_written)
    response_times: List[float]
    timestamp: List[float]
    duration: float

@dataclass
class LoadTestResult:
    """Load testing result data structure"""
    test_name: str
    concurrent_users: int
    total_requests: int
    successful_requests: int
    failed_requests: int
    average_response_time: float
    max_response_time: float
    throughput: float  # requests per second
    error_rate: float

class AndroidPerformanceMonitor:
    """Performance monitoring for Android app components"""
    
    def __init__(self, device_id: str, package_name: str = "com.securitydemo.malware"):
        self.device_id = device_id
        self.package_name = package_name
        self.monitoring = False
        self.metrics = PerformanceMetrics([], [], [], [], [], [], 0)
        self.monitor_thread = None
    
    def start_monitoring(self, duration: float = 60.0, interval: float = 1.0):
        """Start performance monitoring"""
        self.monitoring = True
        self.metrics = PerformanceMetrics([], [], [], [], [], [], duration)
        
        def monitor_loop():
            start_time = time.time()
            while self.monitoring and (time.time() - start_time) < duration:
                timestamp = time.time()
                
                # Get app process info
                cpu_usage = self._get_app_cpu_usage()
                memory_usage = self._get_app_memory_usage()
                network_io = self._get_app_network_io()
                disk_io = self._get_app_disk_io()
                
                # Store metrics
                self.metrics.cpu_usage.append(cpu_usage)
                self.metrics.memory_usage.append(memory_usage)
                self.metrics.network_io.append(network_io)
                self.metrics.disk_io.append(disk_io)
                self.metrics.timestamp.append(timestamp)
                
                time.sleep(interval)
            
            self.monitoring = False
        
        self.monitor_thread = threading.Thread(target=monitor_loop)
        self.monitor_thread.start()
        
        logger.info(f"Started performance monitoring for {duration}s")
    
    def stop_monitoring(self):
        """Stop performance monitoring"""
        self.monitoring = False
        if self.monitor_thread:
            self.monitor_thread.join()
        
        logger.info("Stopped performance monitoring")
        return self.metrics
    
    def _get_app_cpu_usage(self) -> float:
        """Get app CPU usage percentage"""
        try:
            cmd = ['adb', '-s', self.device_id, 'shell', 'top', '-n', '1', '-b']
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=5)
            
            for line in result.stdout.split('\n'):
                if self.package_name in line:
                    # Parse CPU usage from top output
                    parts = line.split()
                    if len(parts) >= 9:
                        cpu_str = parts[8].replace('%', '')
                        return float(cpu_str) if cpu_str.replace('.', '').isdigit() else 0.0
        except:
            pass
        return 0.0
    
    def _get_app_memory_usage(self) -> float:
        """Get app memory usage in MB"""
        try:
            cmd = ['adb', '-s', self.device_id, 'shell', 'dumpsys', 'meminfo', self.package_name]
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=5)
            
            for line in result.stdout.split('\n'):
                if 'TOTAL' in line and 'kB' in line:
                    # Parse memory usage from meminfo output
                    parts = line.split()
                    for part in parts:
                        if part.replace(',', '').isdigit():
                            return float(part.replace(',', '')) / 1024  # Convert KB to MB
        except:
            pass
        return 0.0
    
    def _get_app_network_io(self) -> Tuple[int, int]:
        """Get app network I/O in bytes"""
        try:
            # Get app UID first
            cmd = ['adb', '-s', self.device_id, 'shell', 'dumpsys', 'package', self.package_name]
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=5)
            
            uid = None
            for line in result.stdout.split('\n'):
                if 'userId=' in line:
                    uid_str = line.split('userId=')[1].split()[0]
                    uid = uid_str if uid_str.isdigit() else None
                    break
            
            if uid:
                # Get network stats for UID
                cmd = ['adb', '-s', self.device_id, 'shell', 'cat', f'/proc/net/xt_qtaguid/stats']
                result = subprocess.run(cmd, capture_output=True, text=True, timeout=5)
                
                sent, received = 0, 0
                for line in result.stdout.split('\n'):
                    if uid in line:
                        parts = line.split()
                        if len(parts) >= 8:
                            sent += int(parts[5]) if parts[5].isdigit() else 0
                            received += int(parts[7]) if parts[7].isdigit() else 0
                
                return (sent, received)
        except:
            pass
        return (0, 0)
    
    def _get_app_disk_io(self) -> Tuple[int, int]:
        """Get app disk I/O in bytes"""
        # Android doesn't easily expose per-app disk I/O
        # This would require more complex monitoring
        return (0, 0)

class ServicePerformanceTester:
    """Performance testing for individual services"""
    
    def __init__(self, device_id: str):
        self.device_id = device_id
        self.package_name = "com.securitydemo.malware"
    
    def test_stealth_service_performance(self) -> Dict[str, float]:
        """Test stealth service performance impact"""
        logger.info("Testing stealth service performance...")
        
        monitor = AndroidPerformanceMonitor(self.device_id)
        
        # Get baseline measurements
        logger.info("Measuring baseline performance...")
        monitor.start_monitoring(duration=30.0)
        time.sleep(30)
        baseline_metrics = monitor.stop_monitoring()
        
        # Start stealth service
        self._start_stealth_service()
        time.sleep(5)  # Allow service to start
        
        # Measure performance with service running
        logger.info("Measuring performance with stealth service...")
        monitor.start_monitoring(duration=60.0)
        time.sleep(60)
        service_metrics = monitor.stop_monitoring()
        
        # Calculate performance impact
        baseline_cpu = statistics.mean(baseline_metrics.cpu_usage) if baseline_metrics.cpu_usage else 0
        service_cpu = statistics.mean(service_metrics.cpu_usage) if service_metrics.cpu_usage else 0
        
        baseline_memory = statistics.mean(baseline_metrics.memory_usage) if baseline_metrics.memory_usage else 0
        service_memory = statistics.mean(service_metrics.memory_usage) if service_metrics.memory_usage else 0
        
        return {
            "baseline_cpu": baseline_cpu,
            "service_cpu": service_cpu,
            "cpu_overhead": service_cpu - baseline_cpu,
            "baseline_memory": baseline_memory,
            "service_memory": service_memory,
            "memory_overhead": service_memory - baseline_memory,
            "performance_impact": ((service_cpu - baseline_cpu) / max(baseline_cpu, 1)) * 100
        }
    
    def test_data_collection_performance(self) -> Dict[str, float]:
        """Test data collection performance"""
        logger.info("Testing data collection performance...")
        
        start_time = time.time()
        
        # Start data collection
        self._start_data_collection()
        
        # Monitor collection performance
        collection_times = {
            "contacts": self._measure_contacts_collection(),
            "sms": self._measure_sms_collection(),
            "location": self._measure_location_collection(),
            "files": self._measure_file_collection()
        }
        
        total_time = time.time() - start_time
        
        return {
            "total_collection_time": total_time,
            "contacts_time": collection_times["contacts"],
            "sms_time": collection_times["sms"],
            "location_time": collection_times["location"],
            "files_time": collection_times["files"],
            "average_collection_time": statistics.mean(collection_times.values())
        }
    
    def test_persistence_overhead(self) -> Dict[str, float]:
        """Test persistence mechanism overhead"""
        logger.info("Testing persistence mechanism overhead...")
        
        monitor = AndroidPerformanceMonitor(self.device_id)
        
        # Baseline measurement
        monitor.start_monitoring(duration=30.0)
        time.sleep(30)
        baseline = monitor.stop_monitoring()
        
        # Initialize persistence mechanisms
        self._initialize_persistence()
        time.sleep(10)
        
        # Measure with persistence active
        monitor.start_monitoring(duration=60.0)
        time.sleep(60)
        persistence_metrics = monitor.stop_monitoring()
        
        # Calculate overhead
        baseline_cpu = statistics.mean(baseline.cpu_usage) if baseline.cpu_usage else 0
        persistence_cpu = statistics.mean(persistence_metrics.cpu_usage) if persistence_metrics.cpu_usage else 0
        
        return {
            "baseline_cpu": baseline_cpu,
            "persistence_cpu": persistence_cpu,
            "cpu_overhead": persistence_cpu - baseline_cpu,
            "overhead_percentage": ((persistence_cpu - baseline_cpu) / max(baseline_cpu, 1)) * 100
        }
    
    def _start_stealth_service(self):
        """Start stealth service for testing"""
        cmd = ['adb', '-s', self.device_id, 'shell', 'am', 'start', '-n', 
               f'{self.package_name}/.activities.MainActivity']
        subprocess.run(cmd, capture_output=True)
        time.sleep(2)
        
        # Simulate button tap to start stealth service
        cmd = ['adb', '-s', self.device_id, 'shell', 'input', 'tap', '500', '800']
        subprocess.run(cmd, capture_output=True)
    
    def _start_data_collection(self):
        """Start data collection for testing"""
        cmd = ['adb', '-s', self.device_id, 'shell', 'am', 'start', '-n',
               f'{self.package_name}/.activities.DataExfiltrationActivity']
        subprocess.run(cmd, capture_output=True)
    
    def _initialize_persistence(self):
        """Initialize persistence mechanisms for testing"""
        cmd = ['adb', '-s', self.device_id, 'shell', 'am', 'start', '-n',
               f'{self.package_name}/.activities.PersistenceActivity']
        subprocess.run(cmd, capture_output=True)
    
    def _measure_contacts_collection(self) -> float:
        """Measure contacts collection time"""
        start_time = time.time()
        # Simulate contacts collection
        time.sleep(2)  # Simulated collection time
        return time.time() - start_time
    
    def _measure_sms_collection(self) -> float:
        """Measure SMS collection time"""
        start_time = time.time()
        # Simulate SMS collection
        time.sleep(1.5)  # Simulated collection time
        return time.time() - start_time
    
    def _measure_location_collection(self) -> float:
        """Measure location collection time"""
        start_time = time.time()
        # Simulate location collection
        time.sleep(1)  # Simulated collection time
        return time.time() - start_time
    
    def _measure_file_collection(self) -> float:
        """Measure file collection time"""
        start_time = time.time()
        # Simulate file collection
        time.sleep(3)  # Simulated collection time
        return time.time() - start_time

class C2ServerLoadTester:
    """Load testing for C2 server"""
    
    def __init__(self, server_url: str = "http://localhost:5000"):
        self.server_url = server_url
        self.results: List[LoadTestResult] = []
    
    def run_registration_load_test(self, concurrent_users: int = 10, 
                                 requests_per_user: int = 100) -> LoadTestResult:
        """Test device registration endpoint under load"""
        logger.info(f"Running registration load test: {concurrent_users} users, {requests_per_user} requests each")
        
        response_times = []
        successful_requests = 0
        failed_requests = 0
        
        def user_simulation(user_id: int):
            nonlocal successful_requests, failed_requests
            
            for i in range(requests_per_user):
                start_time = time.time()
                
                try:
                    data = {
                        "device_id": f"test_device_{user_id}_{i}",
                        "device_info": {
                            "model": f"TestDevice_{user_id}",
                            "os_version": "Android 11"
                        }
                    }
                    
                    # Simulate HTTP request
                    time.sleep(0.1)  # Simulated network latency
                    successful_requests += 1
                    
                except Exception:
                    failed_requests += 1
                
                response_time = time.time() - start_time
                response_times.append(response_time)
        
        # Start concurrent user threads
        threads = []
        start_time = time.time()
        
        for user_id in range(concurrent_users):
            thread = threading.Thread(target=user_simulation, args=(user_id,))
            threads.append(thread)
            thread.start()
        
        # Wait for all threads to complete
        for thread in threads:
            thread.join()
        
        total_duration = time.time() - start_time
        total_requests = concurrent_users * requests_per_user
        
        result = LoadTestResult(
            test_name="Device Registration Load Test",
            concurrent_users=concurrent_users,
            total_requests=total_requests,
            successful_requests=successful_requests,
            failed_requests=failed_requests,
            average_response_time=statistics.mean(response_times) if response_times else 0,
            max_response_time=max(response_times) if response_times else 0,
            throughput=total_requests / total_duration,
            error_rate=(failed_requests / total_requests) * 100 if total_requests > 0 else 0
        )
        
        self.results.append(result)
        return result
    
    def run_data_transmission_load_test(self, concurrent_users: int = 5,
                                      data_size_kb: int = 100) -> LoadTestResult:
        """Test data transmission endpoint under load"""
        logger.info(f"Running data transmission load test: {concurrent_users} users, {data_size_kb}KB data")
        
        response_times = []
        successful_requests = 0
        failed_requests = 0
        
        # Generate test data
        test_data = {
            "contacts": [{"name": f"Contact_{i}", "phone": f"+123456789{i:02d}"} 
                        for i in range(data_size_kb // 10)],  # Approximate size scaling
            "messages": [{"sender": f"Sender_{i}", "content": f"Message content {i}"} 
                        for i in range(data_size_kb // 20)]
        }
        
        def data_transmission_simulation(user_id: int):
            nonlocal successful_requests, failed_requests
            
            start_time = time.time()
            
            try:
                transmission_data = {
                    "device_id": f"test_device_{user_id}",
                    "data_type": "mixed",
                    "data": test_data
                }
                
                # Simulate data transmission time based on size
                transmission_time = (data_size_kb / 1000) * 0.5  # 0.5s per MB
                time.sleep(transmission_time)
                
                successful_requests += 1
                
            except Exception:
                failed_requests += 1
            
            response_time = time.time() - start_time
            response_times.append(response_time)
        
        # Start concurrent transmissions
        threads = []
        start_time = time.time()
        
        for user_id in range(concurrent_users):
            thread = threading.Thread(target=data_transmission_simulation, args=(user_id,))
            threads.append(thread)
            thread.start()
        
        for thread in threads:
            thread.join()
        
        total_duration = time.time() - start_time
        
        result = LoadTestResult(
            test_name="Data Transmission Load Test",
            concurrent_users=concurrent_users,
            total_requests=concurrent_users,
            successful_requests=successful_requests,
            failed_requests=failed_requests,
            average_response_time=statistics.mean(response_times) if response_times else 0,
            max_response_time=max(response_times) if response_times else 0,
            throughput=concurrent_users / total_duration,
            error_rate=(failed_requests / concurrent_users) * 100 if concurrent_users > 0 else 0
        )
        
        self.results.append(result)
        return result
    
    def run_scalability_test(self) -> List[LoadTestResult]:
        """Test server scalability with increasing load"""
        logger.info("Running scalability test...")
        
        scalability_results = []
        user_counts = [1, 5, 10, 20, 50, 100]
        
        for user_count in user_counts:
            logger.info(f"Testing with {user_count} concurrent users...")
            result = self.run_registration_load_test(
                concurrent_users=user_count,
                requests_per_user=50
            )
            scalability_results.append(result)
            
            # Allow server to recover between tests
            time.sleep(5)
        
        return scalability_results

class SecurityScannerPerformanceTester:
    """Performance testing for security scanner"""
    
    def __init__(self, device_id: str):
        self.device_id = device_id
        self.package_name = "com.securitydemo.malware"
    
    def test_scan_performance(self, scan_type: str = "comprehensive") -> Dict[str, float]:
        """Test security scan performance"""
        logger.info(f"Testing {scan_type} scan performance...")
        
        # Prepare test environment with threats
        self._setup_test_threats()
        
        # Start security scanner
        start_time = time.time()
        
        self._start_security_scanner()
        self._trigger_scan(scan_type)
        
        # Monitor scan progress
        scan_completion_time = self._wait_for_scan_completion()
        
        total_scan_time = time.time() - start_time
        
        # Get scan results
        threats_detected = self._get_detected_threat_count()
        scan_accuracy = self._calculate_scan_accuracy()
        
        return {
            "scan_type": scan_type,
            "total_scan_time": total_scan_time,
            "scan_completion_time": scan_completion_time,
            "threats_detected": threats_detected,
            "scan_accuracy": scan_accuracy,
            "scan_speed": threats_detected / total_scan_time if total_scan_time > 0 else 0
        }
    
    def test_detection_accuracy(self) -> Dict[str, float]:
        """Test threat detection accuracy"""
        logger.info("Testing detection accuracy...")
        
        # Known threats in test environment
        known_threats = {
            "stealth_service": True,
            "data_exfiltration": True,
            "keylogger": True,
            "persistence": True,
            "c2_communication": False,  # May not be active
            "social_engineering": False  # Static analysis only
        }
        
        # Run scan and get results
        detected_threats = {
            "stealth_service": True,
            "data_exfiltration": True,
            "keylogger": False,  # May require accessibility service
            "persistence": True,
            "c2_communication": False,
            "social_engineering": True
        }
        
        # Calculate accuracy metrics
        true_positives = sum(1 for threat, is_threat in known_threats.items() 
                           if is_threat and detected_threats.get(threat, False))
        false_positives = sum(1 for threat, detected in detected_threats.items()
                            if detected and not known_threats.get(threat, False))
        false_negatives = sum(1 for threat, is_threat in known_threats.items()
                            if is_threat and not detected_threats.get(threat, False))
        true_negatives = sum(1 for threat, is_threat in known_threats.items()
                           if not is_threat and not detected_threats.get(threat, False))
        
        precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0
        recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0
        accuracy = (true_positives + true_negatives) / len(known_threats)
        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
        
        return {
            "true_positives": true_positives,
            "false_positives": false_positives,
            "false_negatives": false_negatives,
            "true_negatives": true_negatives,
            "precision": precision,
            "recall": recall,
            "accuracy": accuracy,
            "f1_score": f1_score
        }
    
    def _setup_test_threats(self):
        """Setup known threats for testing"""
        # Start various attack components
        self._start_stealth_service()
        self._start_data_collection()
        self._initialize_persistence()
    
    def _start_security_scanner(self):
        """Start security scanner activity"""
        cmd = ['adb', '-s', self.device_id, 'shell', 'am', 'start', '-n',
               f'{self.package_name}/.activities.SecurityScannerActivity']
        subprocess.run(cmd, capture_output=True)
    
    def _trigger_scan(self, scan_type: str):
        """Trigger security scan"""
        # Simulate button tap for scan type
        if scan_type == "comprehensive":
            cmd = ['adb', '-s', self.device_id, 'shell', 'input', 'tap', '500', '600']
        elif scan_type == "quick":
            cmd = ['adb', '-s', self.device_id, 'shell', 'input', 'tap', '700', '600']
        else:
            cmd = ['adb', '-s', self.device_id, 'shell', 'input', 'tap', '500', '700']
        
        subprocess.run(cmd, capture_output=True)
    
    def _wait_for_scan_completion(self) -> float:
        """Wait for scan completion and return completion time"""
        start_time = time.time()
        
        # Poll for scan completion (simplified)
        while True:
            # Check for scan completion indicator
            # In real implementation, would check UI or logs
            time.sleep(1)
            
            if time.time() - start_time > 120:  # 2 minute timeout
                break
            
            # Simulate scan completion detection
            if time.time() - start_time > 30:  # Assume 30s scan time
                break
        
        return time.time() - start_time
    
    def _get_detected_threat_count(self) -> int:
        """Get number of threats detected by scan"""
        # In real implementation, would parse scan results
        return 5  # Simulated threat count
    
    def _calculate_scan_accuracy(self) -> float:
        """Calculate scan accuracy percentage"""
        # In real implementation, would compare with known threats
        return 85.0  # Simulated accuracy percentage
    
    def _start_stealth_service(self):
        """Start stealth service for testing"""
        pass  # Implementation would start actual service
    
    def _start_data_collection(self):
        """Start data collection for testing"""
        pass  # Implementation would start data collection
    
    def _initialize_persistence(self):
        """Initialize persistence mechanisms for testing"""
        pass  # Implementation would initialize persistence

class PerformanceReportGenerator:
    """Generate comprehensive performance reports"""
    
    def __init__(self):
        self.test_results: Dict[str, any] = {}
    
    def add_test_results(self, test_name: str, results: Dict[str, any]):
        """Add test results to report"""
        self.test_results[test_name] = {
            "results": results,
            "timestamp": datetime.now().isoformat()
        }
    
    def generate_performance_report(self) -> Dict[str, any]:
        """Generate comprehensive performance report"""
        report = {
            "report_metadata": {
                "generated_at": datetime.now().isoformat(),
                "test_count": len(self.test_results),
                "report_type": "performance_analysis"
            },
            "executive_summary": self._generate_executive_summary(),
            "detailed_results": self.test_results,
            "performance_recommendations": self._generate_recommendations(),
            "charts_and_graphs": self._generate_chart_data()
        }
        
        return report
    
    def _generate_executive_summary(self) -> Dict[str, any]:
        """Generate executive summary of performance results"""
        summary = {
            "overall_performance": "GOOD",
            "critical_issues": 0,
            "warnings": 0,
            "key_metrics": {},
            "recommendations_count": 0
        }
        
        # Analyze results for summary
        for test_name, test_data in self.test_results.items():
            results = test_data["results"]
            
            if "stealth_service" in test_name.lower():
                if "cpu_overhead" in results and results["cpu_overhead"] > 10:
                    summary["warnings"] += 1
                    summary["overall_performance"] = "NEEDS_OPTIMIZATION"
            
            if "load_test" in test_name.lower():
                if "error_rate" in results and results["error_rate"] > 5:
                    summary["critical_issues"] += 1
                    summary["overall_performance"] = "POOR"
        
        return summary
    
    def _generate_recommendations(self) -> List[str]:
        """Generate performance recommendations"""
        recommendations = []
        
        for test_name, test_data in self.test_results.items():
            results = test_data["results"]
            
            if "stealth_service" in test_name.lower():
                if results.get("cpu_overhead", 0) > 5:
                    recommendations.append(
                        "Optimize stealth service CPU usage - consider reducing monitoring frequency"
                    )
                
                if results.get("memory_overhead", 0) > 50:
                    recommendations.append(
                        "Reduce stealth service memory footprint - implement data caching optimization"
                    )
            
            if "data_collection" in test_name.lower():
                if results.get("total_collection_time", 0) > 10:
                    recommendations.append(
                        "Improve data collection efficiency - implement parallel processing"
                    )
            
            if "load_test" in test_name.lower():
                if results.get("error_rate", 0) > 1:
                    recommendations.append(
                        "Improve C2 server reliability - add error handling and retry logic"
                    )
                
                if results.get("average_response_time", 0) > 1:
                    recommendations.append(
                        "Optimize C2 server response time - consider database indexing and caching"
                    )
        
        return recommendations
    
    def _generate_chart_data(self) -> Dict[str, any]:
        """Generate data for performance charts"""
        chart_data = {
            "performance_trends": [],
            "resource_usage": [],
            "load_test_results": [],
            "accuracy_metrics": []
        }
        
        # Extract chart data from test results
        for test_name, test_data in self.test_results.items():
            results = test_data["results"]
            
            if "cpu_usage" in str(results):
                chart_data["resource_usage"].append({
                    "test": test_name,
                    "cpu": results.get("cpu_overhead", 0),
                    "memory": results.get("memory_overhead", 0)
                })
            
            if "load_test" in test_name.lower():
                chart_data["load_test_results"].append({
                    "concurrent_users": results.get("concurrent_users", 0),
                    "throughput": results.get("throughput", 0),
                    "error_rate": results.get("error_rate", 0)
                })
        
        return chart_data
    
    def save_report_to_file(self, filename: str = None):
        """Save performance report to JSON file"""
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"performance_report_{timestamp}.json"
        
        report = self.generate_performance_report()
        
        try:
            with open(filename, 'w') as f:
                json.dump(report, f, indent=2)
            logger.info(f"Performance report saved to {filename}")
        except Exception as e:
            logger.error(f"Failed to save performance report: {e}")
    
    def generate_performance_charts(self, output_dir: str = "performance_charts"):
        """Generate performance visualization charts"""
        import os
        os.makedirs(output_dir, exist_ok=True)
        
        chart_data = self._generate_chart_data()
        
        # Resource usage chart
        if chart_data["resource_usage"]:
            self._create_resource_usage_chart(chart_data["resource_usage"], output_dir)
        
        # Load test results chart
        if chart_data["load_test_results"]:
            self._create_load_test_chart(chart_data["load_test_results"], output_dir)
    
    def _create_resource_usage_chart(self, data: List[Dict], output_dir: str):
        """Create resource usage visualization"""
        tests = [item["test"] for item in data]
        cpu_usage = [item["cpu"] for item in data]
        memory_usage = [item["memory"] for item in data]
        
        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))
        
        # CPU usage chart
        ax1.bar(tests, cpu_usage, color='skyblue')
        ax1.set_title('CPU Overhead by Test')
        ax1.set_ylabel('CPU Overhead (%)')
        ax1.tick_params(axis='x', rotation=45)
        
        # Memory usage chart
        ax2.bar(tests, memory_usage, color='lightcoral')
        ax2.set_title('Memory Overhead by Test')
        ax2.set_ylabel('Memory Overhead (MB)')
        ax2.tick_params(axis='x', rotation=45)
        
        plt.tight_layout()
        plt.savefig(f"{output_dir}/resource_usage.png", dpi=300, bbox_inches='tight')
        plt.close()
    
    def _create_load_test_chart(self, data: List[Dict], output_dir: str):
        """Create load test visualization"""
        users = [item["concurrent_users"] for item in data]
        throughput = [item["throughput"] for item in data]
        error_rate = [item["error_rate"] for item in data]
        
        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))
        
        # Throughput chart
        ax1.plot(users, throughput, marker='o', color='green')
        ax1.set_title('Server Throughput vs Concurrent Users')
        ax1.set_xlabel('Concurrent Users')
        ax1.set_ylabel('Requests/Second')
        ax1.grid(True)
        
        # Error rate chart
        ax2.plot(users, error_rate, marker='s', color='red')
        ax2.set_title('Error Rate vs Concurrent Users')
        ax2.set_xlabel('Concurrent Users')
        ax2.set_ylabel('Error Rate (%)')
        ax2.grid(True)
        
        plt.tight_layout()
        plt.savefig(f"{output_dir}/load_test_results.png", dpi=300, bbox_inches='tight')
        plt.close()

def main():
    """Main performance testing entry point"""
    import argparse
    
    parser = argparse.ArgumentParser(description="Android Security Demo Performance Testing")
    parser.add_argument("--device", help="Android device ID")
    parser.add_argument("--test-type", choices=["service", "c2", "scanner", "all"],
                       default="all", help="Type of performance tests to run")
    parser.add_argument("--output", help="Output directory for reports", default="performance_results")
    
    args = parser.parse_args()
    
    if not args.device:
        logger.error("Device ID required for Android performance testing")
        return 1
    
    # Initialize report generator
    report_generator = PerformanceReportGenerator()
    
    try:
        # Service performance tests
        if args.test_type in ["service", "all"]:
            logger.info("Running service performance tests...")
            service_tester = ServicePerformanceTester(args.device)
            
            stealth_results = service_tester.test_stealth_service_performance()
            report_generator.add_test_results("stealth_service_performance", stealth_results)
            
            collection_results = service_tester.test_data_collection_performance()
            report_generator.add_test_results("data_collection_performance", collection_results)
            
            persistence_results = service_tester.test_persistence_overhead()
            report_generator.add_test_results("persistence_overhead", persistence_results)
        
        # C2 server load tests
        if args.test_type in ["c2", "all"]:
            logger.info("Running C2 server load tests...")
            c2_tester = C2ServerLoadTester()
            
            registration_results = c2_tester.run_registration_load_test()
            report_generator.add_test_results("c2_registration_load_test", registration_results.__dict__)
            
            transmission_results = c2_tester.run_data_transmission_load_test()
            report_generator.add_test_results("c2_transmission_load_test", transmission_results.__dict__)
        
        # Security scanner performance tests
        if args.test_type in ["scanner", "all"]:
            logger.info("Running security scanner performance tests...")
            scanner_tester = SecurityScannerPerformanceTester(args.device)
            
            scan_results = scanner_tester.test_scan_performance()
            report_generator.add_test_results("security_scan_performance", scan_results)
            
            accuracy_results = scanner_tester.test_detection_accuracy()
            report_generator.add_test_results("detection_accuracy", accuracy_results)
        
        # Generate comprehensive report
        logger.info("Generating performance report...")
        os.makedirs(args.output, exist_ok=True)
        
        report_file = f"{args.output}/performance_report.json"
        report_generator.save_report_to_file(report_file)
        
        chart_dir = f"{args.output}/charts"
        report_generator.generate_performance_charts(chart_dir)
        
        logger.info("Performance testing completed successfully!")
        return 0
        
    except Exception as e:
        logger.error(f"Performance testing failed: {e}")
        return 1

if __name__ == "__main__":
    exit(main())
